{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMWxokl4SqRp"
      },
      "source": [
        "# Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WYLMQqTwSqRh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.metrics import SparseCategoricalAccuracy, CategoricalAccuracy\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.densenet import DenseNet201\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import adam_v2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, average_precision_score, roc_auc_score, mean_squared_error, f1_score, confusion_matrix\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "import os \n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1TQHmtHG_p",
        "outputId": "86b7ff64-c0f5-4328-a714-a990a5472570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XyZk9oviHKWZ"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/WIKM')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Bone"
      ],
      "metadata": {
        "id": "jFDo5m4kGbti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pmvRp3DXwHAr"
      },
      "outputs": [],
      "source": [
        "X_train = np.load('X_train_bone.npy')\n",
        "Y_train = np.load('Y_train_bone.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle train data\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train= X_train[s]\n",
        "Y_train = Y_train[s]"
      ],
      "metadata": {
        "id": "NZHMuTHVGVa6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Lung"
      ],
      "metadata": {
        "id": "jlyFs9KIGdgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load arrays RAW\n",
        "\n",
        "X_train_raw = np.load('X_train_raw.npy')\n",
        "Y_train_raw = np.load('Y_train_raw.npy')"
      ],
      "metadata": {
        "id": "Z5wwoGUaJ_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaEyyupZRm4Y"
      },
      "outputs": [],
      "source": [
        "#from patches\n",
        "\n",
        "X_train = np.load('X_train_lung.npy')\n",
        "Y_train = np.load('Y_train_lung.npy')\n",
        "X_test = np.load('X_test_lung.npy')\n",
        "Y_test = np.load('Y_test_lung.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle train data\n",
        "\n",
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "Y_train = Y_train[s]"
      ],
      "metadata": {
        "id": "8ZcOCCQAGZYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle train data RAW\n",
        "\n",
        "s = np.arange(X_train_raw.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train_raw = X_train_raw[s]\n",
        "Y_train_raw = Y_train_raw[s]"
      ],
      "metadata": {
        "id": "yWqQzpR_OD2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle test data\n",
        "\n",
        "s = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_test = X_test[s]\n",
        "Y_test = Y_test[s]"
      ],
      "metadata": {
        "id": "1ocjijWzGaat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluation Split"
      ],
      "metadata": {
        "id": "C7LnH785GgeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    X_train, Y_train, \n",
        "    test_size=0.2, \n",
        "    random_state=11\n",
        ")"
      ],
      "metadata": {
        "id": "TRx2B6zgGg1Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    X_train_raw, Y_train_raw, \n",
        "    test_size=0.2, \n",
        "    random_state=11\n",
        ")"
      ],
      "metadata": {
        "id": "b7r25YJSONhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeHBKGkOSqSH"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GzvWwOX5SqSJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 20\n",
        "\n",
        "# Using original generator\n",
        "train_generator = ImageDataGenerator(\n",
        "        zoom_range=2,  # set range for random zoom\n",
        "        rotation_range = 90,\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True,  # randomly flip images\n",
        "    )\n",
        "\n",
        "#using above generator makes ram explode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PiOb_A16SqSP"
      },
      "outputs": [],
      "source": [
        "optimizer = adam_v2.Adam(learning_rate=1e-4)\n",
        "\n",
        "def build_model(backbone, lr=1e-4):\n",
        "    model = Sequential()\n",
        "    model.add(backbone)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(3, activation='softmax'))\n",
        "    \n",
        "    \n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84L6ak7avueZ"
      },
      "source": [
        "### ResNet 152"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz4DtBCxv1W7"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "densenet = ResNet152(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "model = build_model(densenet ,lr = 1e-4)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDH1gE8cHe4K"
      },
      "source": [
        "### Model: Plain CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp955gpjHenP"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, Dropout, Activation, GlobalAveragePooling2D\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "image_size = 224\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, kernel_size=3, activation='relu', padding='same', input_shape=(224, 224, 3)))\n",
        "model.add(Conv2D(16, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(Conv2D(16, kernel_size=3, activation='relu', padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "#model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer= tf.keras.regularizers.l2(0.01)))\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "#model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "#model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer= tf.keras.regularizers.l2(0.01)))\n",
        "model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "#model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Flatten())\n",
        "#model.add(Dense(256, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(0.01)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(128, activation = 'relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWJxxhzwI4mD"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV-DcpEdSqSN"
      },
      "source": [
        "# Model: DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yglwhBU2OoM"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "densenet = DenseNet201(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "model = build_model(densenet ,lr = 1e-4)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inception V3"
      ],
      "metadata": {
        "id": "ctvAq_iFb8ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "densenet = InceptionV3(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "model = build_model(densenet ,lr = 1e-4)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYNmOMjhb-Gy",
        "outputId": "7ce2c6e1-802f-48e3-f7be-87117d700a03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " batch_normalization_94 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 6147      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,817,123\n",
            "Trainable params: 21,778,595\n",
            "Non-trainable params: 38,528\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG19"
      ],
      "metadata": {
        "id": "6P69S3Kra2hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "densenet = VGG19(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "model = build_model(densenet ,lr = 1e-4)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jriWAn8La4uX",
        "outputId": "12379391-40e3-429e-d51d-875e087ab450"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 512)              2048      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,027,971\n",
            "Trainable params: 20,026,947\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfPUdpl02aM6"
      },
      "source": [
        "# VGG16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiNR9yNsSqSZ"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "densenet = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "model = build_model(densenet ,lr = 1e-4)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2M-0LElVA3G"
      },
      "source": [
        "# ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChHUQnHyVCr4",
        "outputId": "dc9efccc-b176-402d-ff93-7f4db80be180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 2048)             8192      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 6147      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,602,051\n",
            "Trainable params: 23,544,835\n",
            "Non-trainable params: 57,216\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "K.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "resnet50 = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "model = build_model(resnet50 ,lr = 1e-4)\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0kte5sieSqSg"
      },
      "outputs": [],
      "source": [
        "# Learning Rate Reducer\n",
        "learn_control = ReduceLROnPlateau(monitor='val_loss', patience =8,\n",
        "                                  verbose=1,factor=0.1, min_lr=1e-7)\n",
        "\n",
        "# Checkpoint\n",
        "filepath=\"vgg19.bone.best.raw.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, mode='min')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_J8fUneSqSl"
      },
      "source": [
        "# Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sWvVqBX8zq5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906fc90a-103b-4cd3-ed34-1534966c4c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.9215 - accuracy: 0.5719\n",
            "Epoch 1: val_accuracy improved from -inf to 0.65550, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 26s 328ms/step - loss: 0.9215 - accuracy: 0.5719 - val_loss: 1.4337 - val_accuracy: 0.6555 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.7590\n",
            "Epoch 2: val_accuracy improved from 0.65550 to 0.75120, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 9s 228ms/step - loss: 0.6178 - accuracy: 0.7590 - val_loss: 0.7393 - val_accuracy: 0.7512 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.5136 - accuracy: 0.8141\n",
            "Epoch 3: val_accuracy improved from 0.75120 to 0.80861, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 9s 228ms/step - loss: 0.5136 - accuracy: 0.8141 - val_loss: 0.6163 - val_accuracy: 0.8086 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.7950\n",
            "Epoch 4: val_accuracy did not improve from 0.80861\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.5259 - accuracy: 0.7950 - val_loss: 0.6824 - val_accuracy: 0.7512 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.8633\n",
            "Epoch 5: val_accuracy improved from 0.80861 to 0.87081, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 10s 250ms/step - loss: 0.3833 - accuracy: 0.8633 - val_loss: 0.3920 - val_accuracy: 0.8708 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.8765\n",
            "Epoch 6: val_accuracy did not improve from 0.87081\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.3483 - accuracy: 0.8765 - val_loss: 0.6867 - val_accuracy: 0.7416 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.8645\n",
            "Epoch 7: val_accuracy did not improve from 0.87081\n",
            "41/41 [==============================] - 9s 213ms/step - loss: 0.3810 - accuracy: 0.8645 - val_loss: 0.7051 - val_accuracy: 0.7560 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8813\n",
            "Epoch 8: val_accuracy did not improve from 0.87081\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.3234 - accuracy: 0.8813 - val_loss: 0.5363 - val_accuracy: 0.8278 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.9161\n",
            "Epoch 9: val_accuracy improved from 0.87081 to 0.93301, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 11s 254ms/step - loss: 0.2831 - accuracy: 0.9161 - val_loss: 0.2248 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2869 - accuracy: 0.9077\n",
            "Epoch 10: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2869 - accuracy: 0.9077 - val_loss: 0.7273 - val_accuracy: 0.7081 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9065\n",
            "Epoch 11: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2786 - accuracy: 0.9065 - val_loss: 0.2431 - val_accuracy: 0.9330 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9137\n",
            "Epoch 12: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.2339 - accuracy: 0.9137 - val_loss: 0.2658 - val_accuracy: 0.9187 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9257\n",
            "Epoch 13: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2115 - accuracy: 0.9257 - val_loss: 0.4678 - val_accuracy: 0.8325 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9029\n",
            "Epoch 14: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2466 - accuracy: 0.9029 - val_loss: 0.2672 - val_accuracy: 0.8900 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9173\n",
            "Epoch 15: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2406 - accuracy: 0.9173 - val_loss: 0.2417 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9113\n",
            "Epoch 16: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2521 - accuracy: 0.9113 - val_loss: 0.3390 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9065\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 17: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2604 - accuracy: 0.9065 - val_loss: 0.4225 - val_accuracy: 0.8756 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.9424\n",
            "Epoch 18: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.2034 - accuracy: 0.9424 - val_loss: 0.2558 - val_accuracy: 0.9139 - lr: 1.0000e-05\n",
            "Epoch 19/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9424\n",
            "Epoch 19: val_accuracy did not improve from 0.93301\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1786 - accuracy: 0.9424 - val_loss: 0.2837 - val_accuracy: 0.9187 - lr: 1.0000e-05\n",
            "Epoch 20/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9376\n",
            "Epoch 20: val_accuracy improved from 0.93301 to 0.94258, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 11s 255ms/step - loss: 0.1812 - accuracy: 0.9376 - val_loss: 0.2200 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9472\n",
            "Epoch 21: val_accuracy did not improve from 0.94258\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1660 - accuracy: 0.9472 - val_loss: 0.2212 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9532\n",
            "Epoch 22: val_accuracy improved from 0.94258 to 0.94737, saving model to vgg19.bone.best.raw.hdf5\n",
            "41/41 [==============================] - 10s 252ms/step - loss: 0.1430 - accuracy: 0.9532 - val_loss: 0.2044 - val_accuracy: 0.9474 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9472\n",
            "Epoch 23: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1628 - accuracy: 0.9472 - val_loss: 0.2004 - val_accuracy: 0.9474 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9568\n",
            "Epoch 24: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1339 - accuracy: 0.9568 - val_loss: 0.1998 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9604\n",
            "Epoch 25: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1436 - accuracy: 0.9604 - val_loss: 0.1888 - val_accuracy: 0.9474 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1496 - accuracy: 0.9496\n",
            "Epoch 26: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1496 - accuracy: 0.9496 - val_loss: 0.2055 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9532\n",
            "Epoch 27: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1469 - accuracy: 0.9532 - val_loss: 0.1929 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9628\n",
            "Epoch 28: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1280 - accuracy: 0.9628 - val_loss: 0.1986 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9652\n",
            "Epoch 29: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1277 - accuracy: 0.9652 - val_loss: 0.2169 - val_accuracy: 0.9187 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9520\n",
            "Epoch 30: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1304 - accuracy: 0.9520 - val_loss: 0.1871 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9604\n",
            "Epoch 31: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1314 - accuracy: 0.9604 - val_loss: 0.1984 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9616\n",
            "Epoch 32: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1373 - accuracy: 0.9616 - val_loss: 0.1945 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9652\n",
            "Epoch 33: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1085 - accuracy: 0.9652 - val_loss: 0.1827 - val_accuracy: 0.9474 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9652\n",
            "Epoch 34: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.1110 - accuracy: 0.9652 - val_loss: 0.1855 - val_accuracy: 0.9474 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9652\n",
            "Epoch 35: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1295 - accuracy: 0.9652 - val_loss: 0.1842 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9712\n",
            "Epoch 36: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1064 - accuracy: 0.9712 - val_loss: 0.1773 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9700\n",
            "Epoch 37: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1093 - accuracy: 0.9700 - val_loss: 0.1911 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9688\n",
            "Epoch 38: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1034 - accuracy: 0.9688 - val_loss: 0.1819 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9640\n",
            "Epoch 39: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1037 - accuracy: 0.9640 - val_loss: 0.2077 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9652\n",
            "Epoch 40: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1073 - accuracy: 0.9652 - val_loss: 0.1735 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9604\n",
            "Epoch 41: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1169 - accuracy: 0.9604 - val_loss: 0.1888 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9688\n",
            "Epoch 42: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1020 - accuracy: 0.9688 - val_loss: 0.1941 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9700\n",
            "Epoch 43: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1078 - accuracy: 0.9700 - val_loss: 0.4559 - val_accuracy: 0.8086 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9700\n",
            "Epoch 44: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1109 - accuracy: 0.9700 - val_loss: 0.1768 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9724\n",
            "Epoch 45: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0984 - accuracy: 0.9724 - val_loss: 0.1769 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9688\n",
            "Epoch 46: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1087 - accuracy: 0.9688 - val_loss: 0.1845 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9820\n",
            "Epoch 47: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0822 - accuracy: 0.9820 - val_loss: 0.1725 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9712\n",
            "Epoch 48: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.1022 - accuracy: 0.9712 - val_loss: 0.1951 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9712\n",
            "Epoch 49: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0972 - accuracy: 0.9712 - val_loss: 0.1820 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9760\n",
            "Epoch 50: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0858 - accuracy: 0.9760 - val_loss: 0.1982 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9748\n",
            "Epoch 51: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.0928 - accuracy: 0.9748 - val_loss: 0.1800 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9700\n",
            "Epoch 52: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0946 - accuracy: 0.9700 - val_loss: 0.1601 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9748\n",
            "Epoch 53: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0799 - accuracy: 0.9748 - val_loss: 0.1731 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9796\n",
            "Epoch 54: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0830 - accuracy: 0.9796 - val_loss: 0.1851 - val_accuracy: 0.9474 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9796\n",
            "Epoch 55: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0808 - accuracy: 0.9796 - val_loss: 0.1893 - val_accuracy: 0.9426 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9784\n",
            "Epoch 56: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0766 - accuracy: 0.9784 - val_loss: 0.1995 - val_accuracy: 0.9139 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9796\n",
            "Epoch 57: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0873 - accuracy: 0.9796 - val_loss: 0.1878 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9724\n",
            "Epoch 58: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0867 - accuracy: 0.9724 - val_loss: 0.1819 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Epoch 59/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9844\n",
            "Epoch 59: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.0709 - accuracy: 0.9844 - val_loss: 0.2150 - val_accuracy: 0.9187 - lr: 1.0000e-05\n",
            "Epoch 60/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9832\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 60: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0679 - accuracy: 0.9832 - val_loss: 0.2053 - val_accuracy: 0.9378 - lr: 1.0000e-05\n",
            "Epoch 61/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9820\n",
            "Epoch 61: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.0711 - accuracy: 0.9820 - val_loss: 0.1842 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
            "Epoch 62/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9832\n",
            "Epoch 62: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0630 - accuracy: 0.9832 - val_loss: 0.1857 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 63/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9868\n",
            "Epoch 63: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0571 - accuracy: 0.9868 - val_loss: 0.1885 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 64/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9856\n",
            "Epoch 64: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0477 - accuracy: 0.9856 - val_loss: 0.1906 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 65/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9868\n",
            "Epoch 65: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0526 - accuracy: 0.9868 - val_loss: 0.1900 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 66/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9820\n",
            "Epoch 66: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0615 - accuracy: 0.9820 - val_loss: 0.1897 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 67/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9904\n",
            "Epoch 67: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0456 - accuracy: 0.9904 - val_loss: 0.1873 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 68/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9808\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\n",
            "Epoch 68: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0750 - accuracy: 0.9808 - val_loss: 0.1865 - val_accuracy: 0.9426 - lr: 1.0000e-06\n",
            "Epoch 69/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9868\n",
            "Epoch 69: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0565 - accuracy: 0.9868 - val_loss: 0.1869 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 70/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9856\n",
            "Epoch 70: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0608 - accuracy: 0.9856 - val_loss: 0.1865 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 71/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9856\n",
            "Epoch 71: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0560 - accuracy: 0.9856 - val_loss: 0.1869 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 72/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9940\n",
            "Epoch 72: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0520 - accuracy: 0.9940 - val_loss: 0.1869 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 73/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9844\n",
            "Epoch 73: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0654 - accuracy: 0.9844 - val_loss: 0.1877 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 74/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9952\n",
            "Epoch 74: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0465 - accuracy: 0.9952 - val_loss: 0.1876 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 75/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9880\n",
            "Epoch 75: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0504 - accuracy: 0.9880 - val_loss: 0.1872 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 76/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9808\n",
            "Epoch 76: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0661 - accuracy: 0.9808 - val_loss: 0.1872 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 77/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9880\n",
            "Epoch 77: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0488 - accuracy: 0.9880 - val_loss: 0.1864 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 78/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9904\n",
            "Epoch 78: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0474 - accuracy: 0.9904 - val_loss: 0.1868 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 79/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9928\n",
            "Epoch 79: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0517 - accuracy: 0.9928 - val_loss: 0.1870 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 80/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9832\n",
            "Epoch 80: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0625 - accuracy: 0.9832 - val_loss: 0.1870 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 81/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9868\n",
            "Epoch 81: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0595 - accuracy: 0.9868 - val_loss: 0.1875 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 82/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9856\n",
            "Epoch 82: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0559 - accuracy: 0.9856 - val_loss: 0.1875 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 83/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9844\n",
            "Epoch 83: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0649 - accuracy: 0.9844 - val_loss: 0.1877 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 84/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9916\n",
            "Epoch 84: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0519 - accuracy: 0.9916 - val_loss: 0.1873 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 85/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9832\n",
            "Epoch 85: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0607 - accuracy: 0.9832 - val_loss: 0.1869 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 86/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9820\n",
            "Epoch 86: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0547 - accuracy: 0.9820 - val_loss: 0.1882 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 87/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9856\n",
            "Epoch 87: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0508 - accuracy: 0.9856 - val_loss: 0.1878 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 88/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9916\n",
            "Epoch 88: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0466 - accuracy: 0.9916 - val_loss: 0.1876 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 89/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9904\n",
            "Epoch 89: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0469 - accuracy: 0.9904 - val_loss: 0.1873 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 90/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9904\n",
            "Epoch 90: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0472 - accuracy: 0.9904 - val_loss: 0.1871 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 91/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9928\n",
            "Epoch 91: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0485 - accuracy: 0.9928 - val_loss: 0.1870 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 92/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9940\n",
            "Epoch 92: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0417 - accuracy: 0.9940 - val_loss: 0.1878 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 93/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9928\n",
            "Epoch 93: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0444 - accuracy: 0.9928 - val_loss: 0.1881 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 94/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9940\n",
            "Epoch 94: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0402 - accuracy: 0.9940 - val_loss: 0.1875 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 95/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9868\n",
            "Epoch 95: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0575 - accuracy: 0.9868 - val_loss: 0.1874 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 96/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9880\n",
            "Epoch 96: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0529 - accuracy: 0.9880 - val_loss: 0.1874 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 97/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9904\n",
            "Epoch 97: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0528 - accuracy: 0.9904 - val_loss: 0.1878 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 98/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9808\n",
            "Epoch 98: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0699 - accuracy: 0.9808 - val_loss: 0.1880 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 99/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9856\n",
            "Epoch 99: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 209ms/step - loss: 0.0528 - accuracy: 0.9856 - val_loss: 0.1883 - val_accuracy: 0.9426 - lr: 1.0000e-07\n",
            "Epoch 100/100\n",
            "42/41 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9880\n",
            "Epoch 100: val_accuracy did not improve from 0.94737\n",
            "41/41 [==============================] - 9s 210ms/step - loss: 0.0459 - accuracy: 0.9880 - val_loss: 0.1885 - val_accuracy: 0.9426 - lr: 1.0000e-07\n"
          ]
        }
      ],
      "source": [
        "### bone model.fit\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit( x = x_train, y = y_train, batch_size=BATCH_SIZE,\n",
        "      steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
        "      epochs=100,\n",
        "      validation_steps = x_val.shape[0] / BATCH_SIZE,\n",
        "      validation_data=(x_val, y_val),\n",
        "      callbacks=[learn_control, checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajEGVCVOSqSm",
        "outputId": "169ef463-c542-478d-b8eb-ab2c77281c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.8271\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84432, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 90s 297ms/step - loss: 0.4853 - accuracy: 0.8271 - val_loss: 0.4032 - val_accuracy: 0.8443 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9042\n",
            "Epoch 2: val_accuracy did not improve from 0.84432\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.2673 - accuracy: 0.9042 - val_loss: 1.1857 - val_accuracy: 0.7891 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9293\n",
            "Epoch 3: val_accuracy improved from 0.84432 to 0.85997, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 72s 299ms/step - loss: 0.2037 - accuracy: 0.9293 - val_loss: 0.4532 - val_accuracy: 0.8600 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9596\n",
            "Epoch 4: val_accuracy did not improve from 0.85997\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.1050 - accuracy: 0.9596 - val_loss: 2.2421 - val_accuracy: 0.7323 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9711\n",
            "Epoch 5: val_accuracy did not improve from 0.85997\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0917 - accuracy: 0.9711 - val_loss: 0.4483 - val_accuracy: 0.8534 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9734\n",
            "Epoch 6: val_accuracy did not improve from 0.85997\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0833 - accuracy: 0.9734 - val_loss: 1.1705 - val_accuracy: 0.7825 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9720\n",
            "Epoch 7: val_accuracy improved from 0.85997 to 0.91021, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 73s 300ms/step - loss: 0.0801 - accuracy: 0.9720 - val_loss: 0.3482 - val_accuracy: 0.9102 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9845\n",
            "Epoch 8: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 70s 288ms/step - loss: 0.0501 - accuracy: 0.9845 - val_loss: 1.3531 - val_accuracy: 0.7949 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9794\n",
            "Epoch 9: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0767 - accuracy: 0.9794 - val_loss: 0.3849 - val_accuracy: 0.8880 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9798\n",
            "Epoch 10: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0571 - accuracy: 0.9798 - val_loss: 0.4334 - val_accuracy: 0.8764 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9883\n",
            "Epoch 11: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0361 - accuracy: 0.9883 - val_loss: 0.3475 - val_accuracy: 0.8954 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9944\n",
            "Epoch 12: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0193 - accuracy: 0.9944 - val_loss: 0.3915 - val_accuracy: 0.9036 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9850\n",
            "Epoch 13: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0480 - accuracy: 0.9850 - val_loss: 1.9638 - val_accuracy: 0.7611 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9790\n",
            "Epoch 14: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 70s 287ms/step - loss: 0.0708 - accuracy: 0.9790 - val_loss: 0.4418 - val_accuracy: 0.8748 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9843\n",
            "Epoch 15: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0487 - accuracy: 0.9843 - val_loss: 0.3484 - val_accuracy: 0.9003 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9909\n",
            "Epoch 16: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0285 - accuracy: 0.9909 - val_loss: 0.3600 - val_accuracy: 0.8979 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9911\n",
            "Epoch 17: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 70s 289ms/step - loss: 0.0236 - accuracy: 0.9911 - val_loss: 0.5655 - val_accuracy: 0.8633 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9901\n",
            "Epoch 18: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0308 - accuracy: 0.9901 - val_loss: 0.5641 - val_accuracy: 0.8970 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9889\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 19: val_accuracy did not improve from 0.91021\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0317 - accuracy: 0.9889 - val_loss: 0.6317 - val_accuracy: 0.8616 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9928\n",
            "Epoch 20: val_accuracy improved from 0.91021 to 0.91433, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 73s 300ms/step - loss: 0.0193 - accuracy: 0.9928 - val_loss: 0.3156 - val_accuracy: 0.9143 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9977\n",
            "Epoch 21: val_accuracy improved from 0.91433 to 0.91516, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 73s 302ms/step - loss: 0.0144 - accuracy: 0.9977 - val_loss: 0.3280 - val_accuracy: 0.9152 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9988\n",
            "Epoch 22: val_accuracy improved from 0.91516 to 0.91598, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 73s 301ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.3303 - val_accuracy: 0.9160 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9992\n",
            "Epoch 23: val_accuracy improved from 0.91598 to 0.91928, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 73s 301ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.3343 - val_accuracy: 0.9193 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9979\n",
            "Epoch 24: val_accuracy did not improve from 0.91928\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.3299 - val_accuracy: 0.9135 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9957\n",
            "Epoch 25: val_accuracy improved from 0.91928 to 0.92422, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 74s 304ms/step - loss: 0.0182 - accuracy: 0.9957 - val_loss: 0.3564 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n",
            "Epoch 26: val_accuracy did not improve from 0.92422\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.3715 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 27: val_accuracy did not improve from 0.92422\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9209 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 28: val_accuracy did not improve from 0.92422\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
            "Epoch 29: val_accuracy did not improve from 0.92422\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3605 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
            "Epoch 30/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9990\n",
            "Epoch 30: val_accuracy did not improve from 0.92422\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.3516 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
            "Epoch 31/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988\n",
            "Epoch 31: val_accuracy improved from 0.92422 to 0.92504, saving model to resnet50.lung.best.raw.hdf5\n",
            "242/242 [==============================] - 73s 301ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.3538 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
            "Epoch 32/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
            "Epoch 32: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.3605 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
            "Epoch 33/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 33: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3572 - val_accuracy: 0.9226 - lr: 1.0000e-06\n",
            "Epoch 34/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9998\n",
            "Epoch 34: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.3546 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
            "Epoch 35/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
            "Epoch 35: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.3522 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
            "Epoch 36/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9992\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\n",
            "Epoch 36: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.3546 - val_accuracy: 0.9209 - lr: 1.0000e-06\n",
            "Epoch 37/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
            "Epoch 37: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.3538 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 38/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 38: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3519 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 39/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9992\n",
            "Epoch 39: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.3506 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 40/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9992\n",
            "Epoch 40: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.3535 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 41/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 41: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 42/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 9.8420e-04 - accuracy: 1.0000\n",
            "Epoch 42: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 9.8420e-04 - accuracy: 1.0000 - val_loss: 0.3475 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 43/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 43: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3564 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 44/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 44: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3458 - val_accuracy: 0.9209 - lr: 1.0000e-07\n",
            "Epoch 45/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
            "Epoch 45: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3579 - val_accuracy: 0.9250 - lr: 1.0000e-07\n",
            "Epoch 46/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9996\n",
            "Epoch 46: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.3580 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 47/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9992\n",
            "Epoch 47: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0060 - accuracy: 0.9992 - val_loss: 0.3537 - val_accuracy: 0.9250 - lr: 1.0000e-07\n",
            "Epoch 48/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 48: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3619 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 49/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\n",
            "Epoch 49: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.3547 - val_accuracy: 0.9209 - lr: 1.0000e-07\n",
            "Epoch 50/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9990\n",
            "Epoch 50: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.3560 - val_accuracy: 0.9217 - lr: 1.0000e-07\n",
            "Epoch 51/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 51: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 52/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9994\n",
            "Epoch 52: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.3549 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 53/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9988\n",
            "Epoch 53: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3603 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 54/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 54: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3602 - val_accuracy: 0.9242 - lr: 1.0000e-07\n",
            "Epoch 55/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9984\n",
            "Epoch 55: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.3484 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 56/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9994\n",
            "Epoch 56: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0090 - accuracy: 0.9994 - val_loss: 0.3504 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 57/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
            "Epoch 57: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3621 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 58/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
            "Epoch 58: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.3577 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 59/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
            "Epoch 59: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3681 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 60/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 0.9994\n",
            "Epoch 60: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.3660 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 61/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 61: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3589 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 62/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9994\n",
            "Epoch 62: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3518 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 63/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 63: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 64/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 64: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 65/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 65: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3521 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 66/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 66: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3532 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 67/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
            "Epoch 67: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.3514 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 68/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 8.2419e-04 - accuracy: 1.0000\n",
            "Epoch 68: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 8.2419e-04 - accuracy: 1.0000 - val_loss: 0.3601 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 69/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
            "Epoch 69: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.3655 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 70/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9992\n",
            "Epoch 70: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.3549 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 71/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9986\n",
            "Epoch 71: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.3555 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 72/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
            "Epoch 72: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3540 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 73/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
            "Epoch 73: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.3581 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 74/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9998\n",
            "Epoch 74: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.3529 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 75/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.9984\n",
            "Epoch 75: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 70s 287ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.3581 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 76/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 76: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3569 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 77/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 77: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3600 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 78/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9994\n",
            "Epoch 78: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3537 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 79/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9996\n",
            "Epoch 79: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.3588 - val_accuracy: 0.9242 - lr: 1.0000e-07\n",
            "Epoch 80/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 80: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3568 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 81/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 81: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3541 - val_accuracy: 0.9242 - lr: 1.0000e-07\n",
            "Epoch 82/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
            "Epoch 82: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3612 - val_accuracy: 0.9242 - lr: 1.0000e-07\n",
            "Epoch 83/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9996\n",
            "Epoch 83: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.3577 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 84/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9994\n",
            "Epoch 84: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.3613 - val_accuracy: 0.9242 - lr: 1.0000e-07\n",
            "Epoch 85/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.9998\n",
            "Epoch 85: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.3624 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 86/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 86: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3568 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 87/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9992\n",
            "Epoch 87: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.3585 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 88/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
            "Epoch 88: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3515 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 89/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 89: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3546 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 90/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 90: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 91/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\n",
            "Epoch 91: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.3531 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 92/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 92: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3620 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 93/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
            "Epoch 93: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.3573 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 94/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\n",
            "Epoch 94: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.3575 - val_accuracy: 0.9217 - lr: 1.0000e-07\n",
            "Epoch 95/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9988\n",
            "Epoch 95: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.3515 - val_accuracy: 0.9226 - lr: 1.0000e-07\n",
            "Epoch 96/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9998\n",
            "Epoch 96: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 285ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.3563 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 97/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 97: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3518 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 98/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 8.6149e-04 - accuracy: 0.9998\n",
            "Epoch 98: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 8.6149e-04 - accuracy: 0.9998 - val_loss: 0.3616 - val_accuracy: 0.9242 - lr: 1.0000e-07\n",
            "Epoch 99/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.9998\n",
            "Epoch 99: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 70s 288ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.3572 - val_accuracy: 0.9234 - lr: 1.0000e-07\n",
            "Epoch 100/100\n",
            "243/242 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
            "Epoch 100: val_accuracy did not improve from 0.92504\n",
            "242/242 [==============================] - 69s 286ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.3610 - val_accuracy: 0.9242 - lr: 1.0000e-07\n"
          ]
        }
      ],
      "source": [
        "### lung model.fit\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit( x = x_train, y = y_train, batch_size=BATCH_SIZE,\n",
        "      steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n",
        "      epochs=100,\n",
        "      validation_steps = x_val.shape[0] / BATCH_SIZE,\n",
        "      validation_data=(x_val, y_val),\n",
        "      callbacks=[learn_control, checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Vxs-CDC3Q8jw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "36bc5c3d-1bf2-4a81-db2d-49dd23686b16"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot the loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot the accuracy\n",
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXAJgKlxSqSt"
      },
      "outputs": [],
      "source": [
        "with open('history.json', 'w') as f:\n",
        "    json.dump(str(history.history), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_91LlCWSqSy"
      },
      "outputs": [],
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['acc', 'val_acc']].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlBxAgv_SqS3"
      },
      "outputs": [],
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['loss', 'val_loss']].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZcb04DwDM7D"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZJCGQnCSqS8"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGxPt71KSqS9"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"resnet50.lung.best.raw.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr5CdtGLSqTB"
      },
      "outputs": [],
      "source": [
        "Y_val_pred = model.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nipQelbV0VIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2415434-a7e4-4334-8159-9b10a9488226"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1214, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "Y_val_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOtkPPi9kuIC",
        "outputId": "7aa229b6-42b3-4166-cd8f-8b5fcf809034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1214, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "w945Pf73kumT",
        "outputId": "4283e4f3-4126-4441-8caa-93e2b52f0422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c9e394f32906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and multiclass targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))"
      ],
      "metadata": {
        "id": "Lhc3HDK0lCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawPatch = np.load('rawPatch.npy')"
      ],
      "metadata": {
        "id": "BQBQK8UkpCjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawPatch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OmGzaUZpJ_Q",
        "outputId": "ade373d6-2b85-4d76-b5be-9697a1f5bd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_phG3cZRpK96"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "wikmm cancer classification models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}